{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本案例以2020年美国新冠肺炎疫情数据作为数据集，以Python为编程语言，使用Spark对数据进行分析.本实验主要统计以下8个指标，分别是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 统计美国截止每日的累计确诊人数和累计死亡人数。做法是以date作为分组字段，对cases和deaths字段进行汇总统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=SparkConf()).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+-----+------+\n",
      "|     date|     county|     state|cases|deaths|\n",
      "+---------+-----------+----------+-----+------+\n",
      "|2020/1/21|  Snohomish|Washington|    1|     0|\n",
      "|2020/1/22|  Snohomish|Washington|    1|     0|\n",
      "|2020/1/23|  Snohomish|Washington|    1|     0|\n",
      "|2020/1/24|       Cook|  Illinois|    1|     0|\n",
      "|2020/1/24|  Snohomish|Washington|    1|     0|\n",
      "|2020/1/25|     Orange|California|    1|     0|\n",
      "|2020/1/25|       Cook|  Illinois|    1|     0|\n",
      "|2020/1/25|  Snohomish|Washington|    1|     0|\n",
      "|2020/1/26|   Maricopa|   Arizona|    1|     0|\n",
      "|2020/1/26|Los Angeles|California|    1|     0|\n",
      "|2020/1/26|     Orange|California|    1|     0|\n",
      "|2020/1/26|       Cook|  Illinois|    1|     0|\n",
      "|2020/1/26|  Snohomish|Washington|    1|     0|\n",
      "|2020/1/27|   Maricopa|   Arizona|    1|     0|\n",
      "|2020/1/27|Los Angeles|California|    1|     0|\n",
      "|2020/1/27|     Orange|California|    1|     0|\n",
      "|2020/1/27|       Cook|  Illinois|    1|     0|\n",
      "|2020/1/27|  Snohomish|Washington|    1|     0|\n",
      "|2020/1/28|   Maricopa|   Arizona|    1|     0|\n",
      "|2020/1/28|Los Angeles|California|    1|     0|\n",
      "+---------+-----------+----------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"/data/shixunfiles/297b8e9689496aed3d0e9145de9b6884_1606802130422.csv\",inferSchema=\"true\",header=\"true\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = spark.sql(\"select to_date(date.date,'yyyy/MM/dd') as date, sum(cases) as cases, sum(deaths) as deaths from date group by date order by date desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+\n",
      "|      date|  cases|deaths|\n",
      "+----------+-------+------+\n",
      "|2020-05-19|1536471| 91936|\n",
      "|2020-05-18|1515373| 90293|\n",
      "|2020-05-17|1493597| 89504|\n",
      "|2020-05-16|1474612| 88660|\n",
      "|2020-05-15|1450964| 87434|\n",
      "|2020-05-14|1424847| 85842|\n",
      "|2020-05-13|1397833| 84104|\n",
      "|2020-05-12|1376649| 82336|\n",
      "|2020-05-11|1354356| 80681|\n",
      "|2020-05-10|1336598| 79692|\n",
      "|2020-05-09|1316439| 78761|\n",
      "|2020-05-08|1291529| 77306|\n",
      "|2020-05-07|1263875| 75733|\n",
      "|2020-05-06|1235132| 74010|\n",
      "|2020-05-05|1210627| 71066|\n",
      "|2020-05-04|1186913| 68832|\n",
      "|2020-05-03|1164994| 67772|\n",
      "|2020-05-02|1138961| 66444|\n",
      "|2020-05-01|1109434| 64859|\n",
      "|2020-04-30|1075486| 63099|\n",
      "+----------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 统计美国每日的新增确诊人数和新增死亡人数。因为新增数=今日数-昨日数，所以考虑使用自连接，连接条件是t1.date = t2.date + 1，然后使用t1.totalCases – t2.totalCases计算该日新增。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.createOrReplaceTempView(\"data1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+\n",
      "|      date|caseIncrease|deathIncrease|\n",
      "+----------+------------+-------------+\n",
      "|2020-05-19|       21098|         1643|\n",
      "|2020-05-18|       21776|          789|\n",
      "|2020-05-17|       18985|          844|\n",
      "|2020-05-16|       23648|         1226|\n",
      "|2020-05-15|       26117|         1592|\n",
      "|2020-05-14|       27014|         1738|\n",
      "|2020-05-13|       21184|         1768|\n",
      "|2020-05-12|       22293|         1655|\n",
      "|2020-05-11|       17758|          989|\n",
      "|2020-05-10|       20159|          931|\n",
      "|2020-05-09|       24910|         1455|\n",
      "|2020-05-08|       27654|         1573|\n",
      "|2020-05-07|       28743|         1723|\n",
      "|2020-05-06|       24505|         2944|\n",
      "|2020-05-05|       23714|         2234|\n",
      "|2020-05-04|       21919|         1060|\n",
      "|2020-05-03|       26033|         1328|\n",
      "|2020-05-02|       29527|         1585|\n",
      "|2020-05-01|       33948|         1760|\n",
      "|2020-04-30|       30425|         2211|\n",
      "+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = spark.sql(\"select t1.date,t1.cases-t2.cases as caseIncrease,t1.deaths-t2.deaths as deathIncrease from data1 t1,data1 t2 where t1.date = date_add(t2.date,1)\")\n",
    "data2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 统计截止5.19日，美国各州的累计确诊人数和死亡人数。首先筛选出5.19日的数据，然后以state作为分组字段，对cases和deaths字段进行汇总统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+------+\n",
      "|      date|     county|cases|deaths|\n",
      "+----------+-----------+-----+------+\n",
      "|2020-01-21|  Snohomish|    1|     0|\n",
      "|2020-01-22|  Snohomish|    1|     0|\n",
      "|2020-01-23|  Snohomish|    1|     0|\n",
      "|2020-01-24|       Cook|    1|     0|\n",
      "|2020-01-24|  Snohomish|    1|     0|\n",
      "|2020-01-25|     Orange|    1|     0|\n",
      "|2020-01-25|       Cook|    1|     0|\n",
      "|2020-01-25|  Snohomish|    1|     0|\n",
      "|2020-01-26|   Maricopa|    1|     0|\n",
      "|2020-01-26|Los Angeles|    1|     0|\n",
      "|2020-01-26|     Orange|    1|     0|\n",
      "|2020-01-26|       Cook|    1|     0|\n",
      "|2020-01-26|  Snohomish|    1|     0|\n",
      "|2020-01-27|   Maricopa|    1|     0|\n",
      "|2020-01-27|Los Angeles|    1|     0|\n",
      "|2020-01-27|     Orange|    1|     0|\n",
      "|2020-01-27|       Cook|    1|     0|\n",
      "|2020-01-27|  Snohomish|    1|     0|\n",
      "|2020-01-28|   Maricopa|    1|     0|\n",
      "|2020-01-28|Los Angeles|    1|     0|\n",
      "+----------+-----------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allData = spark.sql(\"select to_date(date.date,'yyyy/MM/dd') as date, county, cases, deaths from date\")\n",
    "allData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData.createOrReplaceTempView(\"allData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+\n",
      "|      date|       county|totalCases|totalDeaths|\n",
      "+----------+-------------+----------+-----------+\n",
      "|2020-05-19|New York City|    198710|      20376|\n",
      "|2020-05-19|         Cook|     64722|       2982|\n",
      "|2020-05-19|      Suffolk|     55152|       2619|\n",
      "|2020-05-19|  Los Angeles|     39573|       1913|\n",
      "|2020-05-19|       Nassau|     39361|       2542|\n",
      "|2020-05-19|    Middlesex|     35297|       2470|\n",
      "|2020-05-19|  Westchester|     32401|       1424|\n",
      "|2020-05-19|        Essex|     29513|       2362|\n",
      "|2020-05-19|        Wayne|     21013|       2363|\n",
      "|2020-05-19| Philadelphia|     20129|       1109|\n",
      "|2020-05-19|   Montgomery|     18117|       1158|\n",
      "|2020-05-19|       Hudson|     17621|       1082|\n",
      "|2020-05-19|       Bergen|     17522|       1474|\n",
      "|2020-05-19|       Orange|     16828|        537|\n",
      "|2020-05-19|        Union|     16233|       1023|\n",
      "|2020-05-19|   Miami-Dade|     15941|        578|\n",
      "|2020-05-19|      Passaic|     15371|        842|\n",
      "|2020-05-19|    Fairfield|     14871|       1171|\n",
      "|2020-05-19|    Jefferson|     14371|        855|\n",
      "|2020-05-19|      Unknown|     13594|        550|\n",
      "+----------+-------------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data3 = spark.sql(\"select date, county, sum(cases) as totalCases, sum(deaths) as totalDeaths from allData where date = '2020-05-19' group by date,county order by totalCases desc\")\n",
    "data3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 统计截止5.19日，美国确诊人数最多的十个州。对3)的结果DataFrame注册临时表，然后按确诊人数降序排列，并取前10个州。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.createOrReplaceTempView(\"mostCases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+\n",
      "|      date|       county|totalCases|totalDeaths|\n",
      "+----------+-------------+----------+-----------+\n",
      "|2020-05-19|New York City|    198710|      20376|\n",
      "|2020-05-19|         Cook|     64722|       2982|\n",
      "|2020-05-19|      Suffolk|     55152|       2619|\n",
      "|2020-05-19|  Los Angeles|     39573|       1913|\n",
      "|2020-05-19|       Nassau|     39361|       2542|\n",
      "|2020-05-19|    Middlesex|     35297|       2470|\n",
      "|2020-05-19|  Westchester|     32401|       1424|\n",
      "|2020-05-19|        Essex|     29513|       2362|\n",
      "|2020-05-19|        Wayne|     21013|       2363|\n",
      "|2020-05-19| Philadelphia|     20129|       1109|\n",
      "+----------+-------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Top10 = spark.sql(\"select * from mostCases order by totalCases desc limit 10 \")\n",
    "Top10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) 统计截止5.19日，美国死亡人数最多的十个州。对3)的结果DataFrame注册临时表，然后按死亡人数降序排列，并取前10个州。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+\n",
      "|      date|       county|totalCases|totalDeaths|\n",
      "+----------+-------------+----------+-----------+\n",
      "|2020-05-19|New York City|    198710|      20376|\n",
      "|2020-05-19|         Cook|     64722|       2982|\n",
      "|2020-05-19|      Suffolk|     55152|       2619|\n",
      "|2020-05-19|       Nassau|     39361|       2542|\n",
      "|2020-05-19|    Middlesex|     35297|       2470|\n",
      "|2020-05-19|        Wayne|     21013|       2363|\n",
      "|2020-05-19|        Essex|     29513|       2362|\n",
      "|2020-05-19|  Los Angeles|     39573|       1913|\n",
      "|2020-05-19|       Bergen|     17522|       1474|\n",
      "|2020-05-19|  Westchester|     32401|       1424|\n",
      "+----------+-------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Top10Deaths = spark.sql(\"select * from mostCases order by totalDeaths desc limit 10 \")\n",
    "Top10Deaths.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) 统计截止5.19日，美国确诊人数最少的十个州。对3)的结果DataFrame注册临时表，然后按确诊人数升序排列，并取前10个州。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----------+\n",
      "|      date|     county|totalCases|totalDeaths|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2020-05-19|      Wolfe|         1|          0|\n",
      "|2020-05-19|  Wheatland|         1|          0|\n",
      "|2020-05-19|   Hemphill|         1|          0|\n",
      "|2020-05-19|       Coal|         1|          0|\n",
      "|2020-05-19|   Magoffin|         1|          0|\n",
      "|2020-05-19|      Avery|         1|          0|\n",
      "|2020-05-19|     Harney|         1|          0|\n",
      "|2020-05-19|Piscataquis|         1|          0|\n",
      "|2020-05-19|    Ziebach|         1|          0|\n",
      "|2020-05-19|  Glasscock|         1|          0|\n",
      "+----------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Lowest10Cases = spark.sql(\"select * from mostCases order by totalCases  limit 10 \")\n",
    "Lowest10Cases.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) 统计截止5.19日，美国死亡人数最少的十个州。对3)的结果DataFrame注册临时表，然后按死亡人数升序排列，并取前10个州"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----------+\n",
      "|      date|     county|totalCases|totalDeaths|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2020-05-19|    Trimble|         3|          0|\n",
      "|2020-05-19|     Benzie|         4|          0|\n",
      "|2020-05-19|    Roberts|        22|          0|\n",
      "|2020-05-19|   Glascock|         1|          0|\n",
      "|2020-05-19|      Cooke|        13|          0|\n",
      "|2020-05-19|     Geneva|        23|          0|\n",
      "|2020-05-19|    Prowers|        11|          0|\n",
      "|2020-05-19|   Sheridan|        18|          0|\n",
      "|2020-05-19|   Maverick|        89|          0|\n",
      "|2020-05-19|Buena Vista|       117|          0|\n",
      "+----------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Lowest10Deaths = spark.sql(\"select * from mostCases order by totalDeaths  limit 10 \")\n",
    "Lowest10Deaths.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) 统计截止5.19日，全美和各州的病死率。病死率 = 死亡数/确诊数，对3)的结果DataFrame注册临时表，然后按公式计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|       county|Deathrate|\n",
      "+-------------+---------+\n",
      "|New York City|    0.103|\n",
      "|         Cook|    0.046|\n",
      "|      Suffolk|    0.047|\n",
      "|  Los Angeles|    0.048|\n",
      "|       Nassau|    0.065|\n",
      "|    Middlesex|     0.07|\n",
      "|  Westchester|    0.044|\n",
      "|        Essex|     0.08|\n",
      "|        Wayne|    0.112|\n",
      "| Philadelphia|    0.055|\n",
      "|   Montgomery|    0.064|\n",
      "|       Hudson|    0.061|\n",
      "|       Bergen|    0.084|\n",
      "|       Orange|    0.032|\n",
      "|        Union|    0.063|\n",
      "|   Miami-Dade|    0.036|\n",
      "|      Passaic|    0.055|\n",
      "|    Fairfield|    0.079|\n",
      "|    Jefferson|    0.059|\n",
      "|      Unknown|     0.04|\n",
      "+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roundCases = spark.sql(\"select county, round(totalDeaths/totalCases,3) as Deathrate from mostCases\")\n",
    "roundCases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
